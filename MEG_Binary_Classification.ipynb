{
  "cells": [
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nimport scipy.io as sio\nimport numpy as np\nimport mne\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.svm import LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\nimport torch\nfrom torch.utils.data import Dataset, SubsetRandomSampler, DataLoader\nimport torch.nn as nn\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import ShuffleSplit, cross_val_score\nfrom mne.decoding import CSP\nimport zipfile\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": "['decoding-the-human-brain']\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ba72d02f132206271d0963add8913dd8d76bd924"
      },
      "cell_type": "code",
      "source": "os.listdir('../input/decoding-the-human-brain')",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 2,
          "data": {
            "text/plain": "['test_17_23.zip',\n 'random_submission.csv',\n 'train_07_12.zip',\n 'train_13_16.zip',\n 'train_01_06.zip']"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "95a0c73e5112897c16f325c0c829119534763aea"
      },
      "cell_type": "code",
      "source": "for item in os.listdir('../input/decoding-the-human-brain'):  \n     if 'train' in item:\n        with zipfile.ZipFile(\"../input/decoding-the-human-brain/{}\".format(item),\"r\") as zip_ref:\n            zip_ref.extractall(\"./data/{}\".format(item.split('.')[0]))   #data extraction",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b035bb0438091a6d8c0109ac0bb3a98957cb41e3"
      },
      "cell_type": "code",
      "source": "os.listdir('./data/')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "656687a4851e2dcdaa38b7552bf5fa6af118b586"
      },
      "cell_type": "code",
      "source": "# l = 0\n# for subjectset in os.listdir('./data/'):\n#      if '13' not in subjectset:\n#         for subject in os.listdir('./data/{}/data'.format(subjectset)):\n#             train_mat = sio.loadmat('./data/{}/data/{}'.format(subjectset,subject))\n#             X, Y = np.transpose(train_mat['X'], (0,2,1)), train_mat['y']\n#             if l == 0:\n#                 x_train, y_train = X[:,130:,:], Y\n#                 l = 1\n#             else:\n#                 x_train = np.concatenate([x_train, X[:,130:,:]], axis=0)\n#                 y_train = np.concatenate([y_train, Y], axis=0)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "_uuid": "bdf7c1bf06c8ae07ce2453a74dcaf4d3b290a021"
      },
      "cell_type": "code",
      "source": "# scaler = StandardScaler()\n# s0, s1, s2 = x_train.shape[0], x_train.shape[1], x_train.shape[2]\n# x_train = x_train.reshape(s0,s1*s2)\n# x_train = scaler.fit_transform(x_train)\n# dataset = np.concatenate([x_train, y_train], axis=1)\n# dataset.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0b2e4de146ac35d4d12fffde04b58720922a93f1"
      },
      "cell_type": "code",
      "source": "# val_size = 0.1\n# seed = 42\n# dataset_size = len(dataset)\n# indices = list(range(dataset_size))\n# val_split = int(np.floor(val_size * dataset_size))\n# np.random.seed(seed)\n# np.random.shuffle(indices)\n# train_indices = indices[val_split:]\n# test_indices = indices[:val_split]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "06313523ab0dd87a6b8dcd2d30e16ff16e42cd7f"
      },
      "cell_type": "code",
      "source": "# x_train, y_train = dataset[train_indices, :-1], dataset[train_indices, -1]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4e9dd801023231dd90ee7ed61a88ad79eb52188b"
      },
      "cell_type": "code",
      "source": "# x_test, y_test = dataset[:val_split, :-1], dataset[:val_split, -1]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "56190f34a8d6a631dfefbc93053dab89beefdcec"
      },
      "cell_type": "code",
      "source": "# models = []\n# models.append(('LR', LogisticRegression()))\n# models.append(('KNN3', KNeighborsClassifier(n_neighbors=3)))\n# models.append(('KNN5', KNeighborsClassifier(n_neighbors=5)))\n# models.append(('NB', GaussianNB()))\n# models.append(('SVC', SVC()))\n# models.append(('SVM', SVC(kernel='poly')))\n# models.append(('LSVC', LinearSVC()))\n# models.append(('RFC', RandomForestClassifier()))\n# models.append(('DTC', DecisionTreeClassifier()))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "17e9aa23b23dda518df4bdcbc5d428e60d727069"
      },
      "cell_type": "code",
      "source": "# for name, model in models:\n#     model.fit(x_train, y_train)\n#     predictions = model.predict(x_test)  \n#     acc = np.sum(y_test == predictions) / len(y_test)\n#     print(\"Accuracy for predictions based on {} is : {:.2f}%\".format(name,acc*100))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d6f11e5175f95b71b19985f7b28a3327f67b7207"
      },
      "cell_type": "code",
      "source": "class RawDataset(Dataset):\n    #create dataset for models\n    def __init__(self, train=True):\n        t = 'train' if train else 'test'\n        self.feats, self.labels = self.load_matfiles()\n        self.feats = self.scale(self.feats)\n\n    def load_matfiles(self):\n        l = 0\n        for subjectset in os.listdir('./data/'):\n            for subject in os.listdir('./data/{}/data'.format(subjectset)):\n                train_mat = sio.loadmat('./data/{}/data/{}'.format(subjectset,subject))\n                #we use transpose because we want our data to be batchxseq_lenxfeatures\n                X, Y = np.transpose(train_mat['X'], (0,2,1)), train_mat['y']\n                if l == 0:\n                    x_train, y_train = X[:,130:,:], Y  #we remove the first 0.5 seconds\n                    l = 1\n                else:\n                    x_train = np.concatenate([x_train, X[:,130:,:]], axis=0)\n                    y_train = np.concatenate([y_train, Y], axis=0)\n        return x_train, y_train\n    \n    def scale(self, X):\n        #scaling of the data \n        scaler = StandardScaler()\n        s0, s1, s2 = X.shape[0], X.shape[1], X.shape[2]\n        X = X.reshape(s0,s1 * s2)\n        X = scaler.fit_transform(X)\n        X = X.reshape(s0, s1, s2)\n        return X\n\n    def __getitem__(self, item):\n        return self.feats[item], self.labels[item].astype('float32')\n\n    def __len__(self):\n        return len(self.labels)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4294a02995c91937698328d21ba20318bf3e6c30"
      },
      "cell_type": "code",
      "source": "class ImageDataset(Dataset):\n    def __init__(self, train=True):\n        t = 'train' if train else 'test'\n        self.feats, self.labels = self.load_matfiles()\n        self.feats = self.scale(self.feats)\n\n    def load_matfiles(self):\n        l = 0\n        for subjectset in os.listdir('./data/'):\n            for subject in os.listdir('./data/{}/data'.format(subjectset)):\n                train_mat = sio.loadmat('./data/{}/data/{}'.format(subjectset,subject))\n                X, Y = np.transpose(train_mat['X'], (0,2,1)), train_mat['y']\n                if l == 0:\n                    x_train, y_train = X[:,130:,:], Y\n                    l = 1\n                else:\n                    x_train = np.concatenate([x_train, X[:,130:,:]], axis=0)\n                    y_train = np.concatenate([y_train, Y], axis=0)\n        return x_train, y_train\n    \n    def scale(self, X):\n        scaler = StandardScaler()\n        s0, s1, s2 = X.shape[0], X.shape[1], X.shape[2]\n        X = X.reshape(s0 * s1, s2)\n        X = scaler.fit_transform(X)\n        X = X.reshape(s0, s1, s2)\n        return X\n\n    def __getitem__(self, item):\n        return np.expand_dims(self.feats[item], 0), self.labels[item].astype('float32')\n\n    def __len__(self):\n        return len(self.labels)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b1f65d6db8029dae0dcf35e064de5e91fca019cb"
      },
      "cell_type": "code",
      "source": "def torch_train_val_split(\n        dataset, batch_train, batch_eval,\n        val_size=.2, shuffle=True, seed=42):\n    # Creating data indices for training and validation splits:\n    dataset_size = len(dataset)\n    indices = list(range(dataset_size))\n    val_split = int(np.floor(val_size * dataset_size))\n    if shuffle:\n        np.random.seed(seed)\n        np.random.shuffle(indices)\n    train_indices = indices[val_split:]\n    val_indices = indices[:int(val_split/2)]\n    test_indices = indices[int(val_split/2):val_split]\n    print(\"Training Set size : {}, Validation Set size : {}, Test Set size : {}\".format(len(train_indices), len(val_indices), len(test_indices)))\n    # Creating PT data samplers and loaders:\n    train_sampler = SubsetRandomSampler(train_indices)\n    val_sampler = SubsetRandomSampler(val_indices)\n    test_sampler = SubsetRandomSampler(test_indices)\n\n    train_loader = DataLoader(dataset,\n                              batch_size=batch_train, \n                              sampler=train_sampler)\n    val_loader = DataLoader(dataset,\n                            batch_size=batch_eval ,\n                            sampler=val_sampler)\n    test_loader = DataLoader(dataset,\n                            batch_size=batch_eval ,\n                            sampler=test_sampler)\n    return train_loader, val_loader, test_loader",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "068a37555a9773401cbc27cca2cd3c96796c6a9f"
      },
      "cell_type": "code",
      "source": "raw_data = RawDataset()\ntrain_loader, val_loader, test_loader = torch_train_val_split(raw_data, 32 ,32, val_size=.2, shuffle=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "847f66039c088114bedb0052f9f918ac918966ae"
      },
      "cell_type": "code",
      "source": "for index,item in enumerate(train_loader):\n    print(index, item[0].size(), item[1].size())\n    if index == 3:\n        break",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4e3b0fcea461a8093b0aaedae6873420e6f42d8f"
      },
      "cell_type": "code",
      "source": "def train(train_loader, val_loader, model, criterion, optimizer, max_epochs=50, patience=20):\n    e = 0\n    min_val_loss = 1e10\n    plot_train_loss, plot_val_loss = [], []\n    while patience >= 0 and e < max_epochs:\n        #list of train losses per epoch\n        batch_loss = []\n        model.train()\n        for i_batch, data_batched in enumerate(train_loader, 0):\n            data_batched[0] = data_batched[0].to(0)\n            y_pred = model(data_batched[0]).to(0)\n            #calculate batch loss \n            data_batched[1] = data_batched[1].to(0)\n            loss = criterion(y_pred, data_batched[1])\n            #append loss to list in order to print train loss per epoch\n            batch_loss.append(loss.item())\n            print('Batch loss: {}'.format(loss.item()))\n            # Zero gradients, perform a backward pass, and update the weights.\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n        train_loss = np.mean(batch_loss)\n        plot_train_loss.append(train_loss)\n        print('Train loss for epoch {} : {}'.format(e,train_loss))\n        # set eval mode in order to stop dropout during inference\n        model.eval()\n        val_loss = []\n        for i_batch, data_batched in enumerate(val_loader, 0):\n            data_batched[0]= data_batched[0].to(0)\n            y_pred = model(data_batched[0]).to(0)\n            data_batched[1] = data_batched[1].to(0)\n            loss = criterion(y_pred, data_batched[1])\n            val_loss.append(loss.item()) \n        validation_loss = np.mean(val_loss)\n        plot_val_loss.append(validation_loss)\n        print('Val loss for epoch {} : {}, Best loss : {}, Patience : {}'.format(e, validation_loss, \n                                                                                 min_val_loss, patience))\n        e += 1\n        if validation_loss <= min_val_loss:\n            min_val_loss = validation_loss\n            patience = 20\n            torch.save(model.state_dict(), \"LSTMmodel_params.pt\")\n        else:\n            patience -= 1",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ca7f19f9d38aa04e7f655dacfb7286e2cd302e73"
      },
      "cell_type": "code",
      "source": "def predict(test_loader, model, criterion):\n    predictions, y_test, test_loss = [], [], []\n    model.load_state_dict(torch.load(\"LSTMmodel_params.pt\"))\n    # set eval mode in order to stop dropout during inference\n    model.eval()\n    model.to(0)\n    for i_batch, data_batched in enumerate(test_loader, 0):\n        data_batched[0] = data_batched[0].to(0)\n        y_pred = model(data_batched[0])\n        preds = y_pred.data.cpu().numpy()\n        predictions += list(np.round(preds))\n        # get sorted version of y_test\n        data_batched[1] = data_batched[1].to(0)\n        y_test += list(data_batched[1].data.cpu().numpy())\n        loss = criterion(y_pred, data_batched[1]) \n        test_loss.append(loss.item()) \n    total_loss = np.mean(test_loss)\n    print('Test loss: {}'.format(total_loss))\n    accuracy = np.sum(np.asarray(predictions) == np.asarray(y_test)) / len(y_test)\n    print(\"Test set accuracy is {:.2f}% \".format(accuracy*100))\n    return(predictions, y_test)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4bfc128e5112e3b5e61e1dbadb4ad8b23d45bfe7"
      },
      "cell_type": "code",
      "source": "class DNN(nn.Module):\n    def __init__(self, layer1, layer2, layer3, output_size):\n        super(DNN, self).__init__()\n        self.layer1 = layer1\n        self.layer2 = layer2\n        self.layer3 = layer3\n        self.output_size = output_size\n        self.fc1 = nn.Linear(self.layer1, self.layer2)\n        self.fc2 = nn.Linear(self.layer2,self.layer3)\n        self.fc3 = nn.Linear(self.layer3, self.output_size)\n        self.sigmoid = nn.Sigmoid()\n        \n    def forward(self,x):\n        x = x.view(x.size(0), -1)\n        x1 = self.fc1(x)\n        x2 = self.fc2(x1)\n        x3 = self.fc3(x2)\n        output = self.sigmoid(x3)\n        return output",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "71044bfc41cc29f497f40b1b3185f837eba8d6c5"
      },
      "cell_type": "code",
      "source": "FC1, FC2, FC3, D_out = 245*306, 200, 20, 1\n\nmodel = DNN(FC1, FC2, FC3, D_out)\nmodel.to(0)\n\ncriterion =  nn.BCELoss()\noptimizer = torch.optim.Adam(model.parameters(), weight_decay=0.1)\nmodel.parameters",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a432f7123b01bd0d64c303b90439146bf4830c6b",
        "scrolled": true
      },
      "cell_type": "code",
      "source": "train(train_loader, val_loader, model, criterion, optimizer, max_epochs=50, patience=20)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0bff7aadf47d6cc380e50d3613854c13075a3278"
      },
      "cell_type": "code",
      "source": "predictions, labels = predict(val_loader, model, criterion)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c0a97d3c1e8f1824038773da133466d8f737a893"
      },
      "cell_type": "code",
      "source": "class BasicLSTM(nn.Module):\n    def __init__(self, input_dim, rnn_size, output_dim, num_layers, dropout, bidirectional=False):\n        super(BasicLSTM, self).__init__()\n        self.bidirectional = bidirectional\n        self.feature_size = rnn_size * 2 if self.bidirectional else rnn_size\n        # --------------- Insert your code here ---------------- #\n        # Initialize the LSTM, Dropout, Output layers\n        self.input_size = input_dim\n        self.num_layers= num_layers\n        self.output_size = output_dim\n        self.dropout = dropout\n        self.lstm = nn.LSTM(self.input_size, rnn_size, self.num_layers, dropout=self.dropout, \n                            batch_first=True, bidirectional=self.bidirectional)\n        self.fc = nn.Linear(self.feature_size, self.output_size)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        \"\"\" \n            x : 3D numpy array of dimension N x L x D\n                N: batch index\n                L: sequence index\n                D: feature index\n         \"\"\"\n        lstm_outs, hidden = self.lstm(x)\n        last_out = self.last_timestep(lstm_outs, self.bidirectional)\n        output = self.fc(last_out)\n        output = self.sigmoid(output)\n        return output\n\n    def last_timestep(self, outputs, bidirectional=False):\n        \"\"\"\n            Returns the last output of the LSTM taking into account the zero padding\n        \"\"\"\n        if bidirectional:\n            forward, backward = self.split_directions(outputs)\n            last_forward = forward[:, -1, :]\n            last_backward = backward[:, 0, :]\n            # Concatenate and return - maybe add more functionalities like average\n            return torch.cat((last_forward, last_backward), dim=-1)\n        else:\n            return outputs[:, -1, :]\n\n    @staticmethod\n    def split_directions(outputs):\n        direction_size = int(outputs.size(-1) / 2)\n        forward = outputs[:, :, :direction_size]\n        backward = outputs[:, :, direction_size:]\n        return forward, backward",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a3a4e151ea3f719817db29faa83522434a6cd844",
        "scrolled": true
      },
      "cell_type": "code",
      "source": "D_in, H, D_out, num_layers, dropout, biLSTM = 306, 20, 1, 2, 0.2, True\n\nmodel = BasicLSTM(D_in, H, D_out, num_layers, dropout, biLSTM)\nmodel.to(0)\n\ncriterion =  nn.BCELoss()\noptimizer = torch.optim.Adam(model.parameters(), weight_decay=0.001)\nmodel.parameters",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7bb5141c31c7c6ecf5331f5ae0ef7bc64dc15d85",
        "scrolled": true
      },
      "cell_type": "code",
      "source": "train(train_loader, val_loader, model, criterion, optimizer, max_epochs=50, patience=20)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "512be9f6599b575c2d47b6ade553d8dc52cda313"
      },
      "cell_type": "code",
      "source": "predictions, labels = predict(val_loader, model, criterion)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "541336da2e8681b742f1f08cbf07dc14497d8c44"
      },
      "cell_type": "code",
      "source": "class CLSTM(nn.Module):\n    def __init__(self, input_dim, rnn_size, output_dim, num_layers, dropout, bidirectional=False):\n        super(CLSTM, self).__init__()\n        self.bidirectional = bidirectional\n        self.feature_size = rnn_size * 2 if self.bidirectional else rnn_size\n        # --------------- Insert your code here ---------------- #\n        # Initialize the LSTM, Dropout, Output layers\n        self.input_size = input_dim\n        self.num_layers= num_layers\n        self.output_size = output_dim\n        self.dropout = dropout\n        self.conv1 = nn.Conv1d(in_channels=self.input_size ,out_channels=self.input_size ,kernel_size=3, padding=1)\n        self.pool1 = nn.MaxPool1d(2)\n        self.conv2 = nn.Conv1d(in_channels=self.input_size ,out_channels=self.input_size ,kernel_size=3, padding=1)\n        self.pool2 = nn.MaxPool1d(2)\n#         self.conv3 = nn.Conv1d(in_channels=self.input_size ,out_channels=self.input_size ,kernel_size=3, padding=1)\n#         self.pool3 = nn.MaxPool1d(2)\n        self.lstm = nn.LSTM(self.input_size, rnn_size, self.num_layers, dropout=self.dropout, \n                            batch_first=True, bidirectional=self.bidirectional)\n        self.fc = nn.Linear(self.feature_size, self.output_size)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        \"\"\" \n            x : 3D numpy array of dimension N x L x D\n                N: batch index\n                L: sequence index\n                D: feature index\n         \"\"\"\n        x = x.transpose(1, 2)\n        x = self.conv1(x)\n        x = self.pool1(x)\n        x = self.conv2(x)\n        x = self.pool2(x)\n#         x = self.conv3(x)\n#         x = self.pool3(x)\n        x = x.transpose(1,2)\n        lstm_outs, hidden = self.lstm(x)\n        last_out = self.last_timestep(lstm_outs, self.bidirectional)\n        output = self.fc(last_out)\n        output = self.sigmoid(output)\n        return output\n\n    def last_timestep(self, outputs, bidirectional=False):\n        \"\"\"\n            Returns the last output of the LSTM taking into account the zero padding\n        \"\"\"\n        if bidirectional:\n            forward, backward = self.split_directions(outputs)\n            last_forward = forward[:, -1, :]\n            last_backward = backward[:, 0, :]\n            # Concatenate and return - maybe add more functionalities like average\n            return torch.cat((last_forward, last_backward), dim=-1)\n        else:\n            return last_forward\n\n    @staticmethod\n    def split_directions(outputs):\n        direction_size = int(outputs.size(-1) / 2)\n        forward = outputs[:, :, :direction_size]\n        backward = outputs[:, :, direction_size:]\n        return forward, backward",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8e839ca44d463674a73b71f55be30c77dff6a231"
      },
      "cell_type": "code",
      "source": "D_in, H, D_out, num_layers, dropout, biLSTM = 306, 30, 1, 2, 0.1, True\n\nmodel = CLSTM(D_in, H, D_out, num_layers, dropout, biLSTM)\nmodel.to(0)\n\ncriterion =  nn.BCELoss()\noptimizer = torch.optim.Adam(model.parameters(), weight_decay=0.0001)\nmodel.parameters",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "974386355cb351e10cb1593ba2eb911c80210032",
        "scrolled": true
      },
      "cell_type": "code",
      "source": "train(train_loader, val_loader, model, criterion, optimizer, max_epochs=50, patience=20)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "83a8af1ba2179d692b19070a02beee9ee850f27b"
      },
      "cell_type": "code",
      "source": "predictions, labels = predict(test_loader, model, criterion)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6f0fc1ec6a1f997d916953021a87658874833df7"
      },
      "cell_type": "code",
      "source": "img_data = ImageDataset()\ntrain_loader, val_loader, test_loader = torch_train_val_split(img_data, 32 ,32, val_size=.2, shuffle=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "67042046e2c9a88ee289cd77ed75c21378b900a1"
      },
      "cell_type": "code",
      "source": "class Unit(nn.Module):\n    def __init__(self,in_channels,out_channels):\n        super(Unit,self).__init__()\n        self.conv = nn.Conv2d(in_channels=in_channels,kernel_size=5,out_channels=out_channels,stride=1,padding=2)\n        self.bn = nn.BatchNorm2d(num_features=out_channels)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.relu = nn.ReLU()\n\n    def forward(self,x):\n        output = self.conv(x.float())\n        output = self.bn(output)\n        output = self.relu(output)\n        output = self.pool(output)\n        return output\n    \nclass CNN(nn.Module):\n    def __init__(self, num_class):\n        super(CNN, self).__init__()\n        self.num_class = num_class\n        self.unit1 = Unit(in_channels=1, out_channels=16)\n        self.unit2 = Unit(in_channels=16, out_channels=8)\n        self.unit3 = Unit(in_channels=8, out_channels=4)\n        self.unit4 = Unit(in_channels=4, out_channels=2)\n        self.net = nn.Sequential(self.unit1, self.unit2, self.unit3, self.unit4)\n        self.fc1 = nn.Linear(2*15*19, 1)\n        #self.fc2 = nn.Linear(500, self.num_class)\n        self.sigmoid = nn.Sigmoid()\n    \n    def forward(self, x):\n        output = self.net(x)\n        output = output.view(output.size(0) , -1)\n        output = self.fc1(output)\n        #output = self.fc2(output)\n        output = self.sigmoid(output)\n        return output",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "_uuid": "774af6c38cc1767af50e5dbe3b1aba2c4ca52665"
      },
      "cell_type": "code",
      "source": "model = CNN(1)\nmodel.to(0)\n\ncriterion =  nn.BCELoss()\noptimizer = torch.optim.Adam(model.parameters(), weight_decay=0.005)\nmodel.parameters",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "cc0cb0c8ace928c26e03602a4e561636cc2a05ab"
      },
      "cell_type": "code",
      "source": "train(train_loader, val_loader, model, criterion, optimizer, max_epochs=100, patience=20)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "300b16a811db188437aaad4d008433b3be94a657"
      },
      "cell_type": "code",
      "source": "predictions, labels = predict(test_loader, model, criterion)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "fb7a276d75776750bba9d11236695d93ac24b225"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}